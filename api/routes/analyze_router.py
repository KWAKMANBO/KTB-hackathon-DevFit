import asyncio
import logging
import time
from fastapi import APIRouter, BackgroundTasks, HTTPException
from fastapi.responses import JSONResponse
from db.repositories import *
from schema.request_analyze import *
from services.analyze_service import *
from services.s3_service import *

# LangChain íŒŒì´í”„ë¼ì¸
from apiv2.langchain_pipeline.chains.company_chain import CompanyAnalysisChain
from apiv2.langchain_pipeline.chains.applicant_chain import ApplicantAnalysisChain
from apiv2.langchain_pipeline.chains.compare_chain import CultureCompareChain

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/analyze", tags=["candidates"])

# ë¶„ì„ ìƒíƒœ ì €ì¥ì†Œ (í”„ë¡œë•ì…˜ì—ì„œëŠ” Redis ê¶Œì¥)
analysis_status = {}


# ============================================================
# ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‘ì—…
# ============================================================

async def run_analysis(result_key: str, jd_url: str, s3_keys: list[str]):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (LangChain)"""
    company_chain = None
    applicant_chain = None
    compare_chain = None
    total_start = time.time()

    logger.info(f"{'=' * 60}")
    logger.info(f"ğŸš€ ë¶„ì„ ì‹œì‘ | result_key: {result_key}")
    logger.info(f"   JD URL: {jd_url}")
    logger.info(f"   S3 Keys: {s3_keys}")
    logger.info(f"{'=' * 60}")

    try:
        # ì²´ì¸ ì´ˆê¸°í™” (save_to_db=Trueë¡œ LangChainì—ì„œ ì§ì ‘ DB ì €ì¥)
        logger.info("âš™ï¸  ì²´ì¸ ì´ˆê¸°í™” ì¤‘...")
        company_chain = CompanyAnalysisChain(save_to_db=True)
        applicant_chain = ApplicantAnalysisChain(save_to_db=True)
        compare_chain = CultureCompareChain(save_to_db=True)
        logger.info("âœ… ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ")

        # 1. íšŒì‚¬ + êµ¬ì§ì ë³‘ë ¬ ë¶„ì„
        logger.info(f"\n{'â”€' * 40}")
        logger.info("ğŸš€ [1/3] íšŒì‚¬ + êµ¬ì§ì ë³‘ë ¬ ë¶„ì„ ì‹œì‘")
        logger.info(f"   ğŸ“‹ íšŒì‚¬ JD: {jd_url}")
        logger.info(f"   ğŸ“„ êµ¬ì§ì S3: {s3_keys[0] if s3_keys else 'N/A'}")
        step_start = time.time()

        if not s3_keys:
            raise ValueError("ë¶„ì„í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        analysis_status[result_key] = {
            "status": "processing",
            "step": "parallel_analysis",
            "progress": 10,
            "message": "íšŒì‚¬ + êµ¬ì§ì ë³‘ë ¬ ë¶„ì„ ì¤‘..."
        }

        # ë³‘ë ¬ ì‹¤í–‰
        company_data, candidate_data = await asyncio.gather(
            company_chain.run(jd_url),
            applicant_chain.run_from_s3(s3_keys[0])
        )

        analysis_status[result_key]["progress"] = 70
        logger.info(f"âœ… [1/3] ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ ({time.time() - step_start:.1f}ì´ˆ)")
        logger.info(f"   íšŒì‚¬ëª…: {company_data.get('profile_meta', {}).get('company_name', 'N/A')}")
        logger.info(f"   ì§€ì›ìëª…: {candidate_data.get('profile_meta', {}).get('candidate_name', 'N/A')}")

        # 2. ì»¬ì³í• ë§¤ì¹­
        logger.info(f"\n{'â”€' * 40}")
        logger.info("ğŸ”„ [2/3] ì»¬ì³í• ë§¤ì¹­ ì‹œì‘")
        step_start = time.time()
        analysis_status[result_key].update({
            "step": "culture_fit",
            "progress": 80,
            "message": "ì»¬ì³í• ë§¤ì¹­ ì¤‘..."
        })
        matching_result = await compare_chain.run(company_data, candidate_data)
        analysis_status[result_key]["progress"] = 95
        logger.info(f"âœ… [2/3] ì»¬ì³í• ë§¤ì¹­ ì™„ë£Œ ({time.time() - step_start:.1f}ì´ˆ)")
        logger.info(f"   ë§¤ì¹­ ì ìˆ˜: {matching_result.get('overall', {}).get('match_score', 'N/A')}")

        # 3. DB ì €ì¥ ì™„ë£Œ í™•ì¸ (LangChainì—ì„œ ì´ë¯¸ ì €ì¥ë¨)
        logger.info(f"\n{'â”€' * 40}")
        logger.info("ğŸ’¾ [3/3] DB ì €ì¥ ì™„ë£Œ (LangChain ë‚´ë¶€ì—ì„œ ì €ì¥ë¨)")
        company_id = company_data.get("_id", "N/A")
        candidate_id = candidate_data.get("_id", "N/A")
        matching_id = matching_result.get("_id", "N/A")
        logger.info(f"   company_id: {company_id}")
        logger.info(f"   candidate_id: {candidate_id}")
        logger.info(f"   matching_id: {matching_id}")

        # 4. ì™„ë£Œ
        analysis_status[result_key] = {
            "status": "completed",
            "step": "done",
            "progress": 100,
            "message": "ë¶„ì„ ì™„ë£Œ",
            "result": {
                "company_id": company_id,
                "candidate_id": candidate_id,
                "matching_id": matching_id,
                "company": company_data,
                "candidate": candidate_data,
                "culture_fit": matching_result
            }
        }

        logger.info(f"\n{'=' * 60}")
        logger.info(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {time.time() - total_start:.1f}ì´ˆ")
        logger.info(f"{'=' * 60}\n")

    except Exception as e:
        logger.error(f"\n{'=' * 60}")
        logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        logger.error(f"{'=' * 60}\n")
        analysis_status[result_key] = {
            "status": "failed",
            "step": "error",
            "progress": 0,
            "message": f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        }

    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        logger.info("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        if company_chain:
            company_chain.close()
        if applicant_chain:
            applicant_chain.close()
        if compare_chain:
            compare_chain.close()
        logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# ============================================================
# API ì—”ë“œí¬ì¸íŠ¸
# ============================================================

@router.post("/upload")
async def upload(data: RequestAnalyze):
    """1ë‹¨ê³„: Presigned URL ë°œê¸‰"""
    presigned_urls = []
    result_key = generate_result_key()

    logger.info(f"ğŸ“¤ Upload ìš”ì²­ | JD URL: {data.jd_url}")
    logger.info(f"   íŒŒì¼ ìˆ˜: {len(data.files)}")

    # S3 í‚¤ ëª©ë¡ ìƒì„±
    s3_keys = []
    for f in data.files:
        # s3_key = f"{result_key}/{f.file_name}"
        # s3_keys.append(s3_key)
        presigned_urls.append(generated_presigned_url(result_key, f.file_name, f.content_type))
        logger.info(f"   - {f.file_name} ({f.content_type})")

    # ìƒíƒœ ì´ˆê¸°í™” (s3_keys í¬í•¨)
    analysis_status[result_key] = {
        "status": "pending",
        "step": "upload",
        "progress": 0,
        "message": "íŒŒì¼ ì—…ë¡œë“œ ëŒ€ê¸° ì¤‘...",
        "jd_url": data.jd_url,
        # "s3_keys": s3_keys
    }

    # print(analysis_status)

    logger.info(f"âœ… result_key ë°œê¸‰: {result_key}")

    return {
        "result_key": result_key,
        "presigned_urls": presigned_urls
    }


@router.post("/start/{result_key}")
# @router.post("/start")
async def start_analysis(result_key: str, background_tasks: BackgroundTasks):
# # async def start_analysis(background_tasks: BackgroundTasks):
#     result_key = 'd1bb78a6-fad5-4583-8f51-c68e989ef059'
#     analysis_status['d1bb78a6-fad5-4583-8f51-c68e989ef059'] = {'status': 'pending', 'step': 'upload', 'progress': 0,
#                                                                'message': 'íŒŒì¼ ì—…ë¡œë“œ ëŒ€ê¸° ì¤‘...',
#                                                                'jd_url': 'https://toss.im/career/jobs/4829381'}

    """2ë‹¨ê³„: íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ í›„ ë¶„ì„ ì‹œì‘"""
    if result_key not in analysis_status:
        raise HTTPException(status_code=404, detail="result_key not found")

    jd_url = analysis_status[result_key].get("jd_url", "")

    # S3ì—ì„œ 'result_key/' prefixë¥¼ ê°€ì§„ íŒŒì¼ ëª©ë¡ì„ ì§ì ‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
    s3_keys = list_files_in_prefix(result_key)

    if not s3_keys:
        # S3ì— íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¶„ì„ì„ ì‹œì‘í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì˜¤ë¥˜ ì²˜ë¦¬
        raise HTTPException(status_code=400, detail="S3ì— ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹¤í–‰
    background_tasks.add_task(run_analysis, result_key, jd_url, s3_keys)

    # ìƒíƒœ ì—…ë°ì´íŠ¸: ë¶„ì„ ì‹œì‘ë¨ì„ ëª…ì‹œí•˜ê³ , ì°¾ì€ s3_keysë¥¼ ì €ì¥
    analysis_status[result_key].update({
        "status": "started",
        "step": "analysis_start",
        "progress": 5,
        "message": "ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "s3_keys": s3_keys
    })

    return {
        "result_key": result_key,
        "status": "started",
        "message": "ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
    }


@router.get("/status/{result_key}")
async def get_status(result_key: str, timeout: int = 15):
    """3ë‹¨ê³„: Long Pollingìœ¼ë¡œ ìƒíƒœ í™•ì¸

    Response Status Codes:
        200: ë¶„ì„ ì™„ë£Œ (completed)
        202: ë¶„ì„ ì§„í–‰ ì¤‘ (processing/timeout)
        404: result_keyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ
        500: ë¶„ì„ ì‹¤íŒ¨ (failed)
    """
    if result_key not in analysis_status:
        raise HTTPException(status_code=404, detail="result_key not found")

    elapsed = 0
    poll_interval = 1

    while elapsed < timeout:
        status = analysis_status.get(result_key)

        # ì™„ë£Œ ì‹œ 200 OK ë°˜í™˜ - MongoDBì—ì„œ ì¡°íšŒí•˜ì—¬ ë°˜í™˜
        if status["status"] == "completed":
            result = status.get("result", {})
            company_id = result.get("company_id")
            candidate_id = result.get("candidate_id")
            matching_id = result.get("matching_id")

            # MongoDBì—ì„œ ê° ë°ì´í„° ì¡°íšŒ
            company_analysis = await company_repository.get_company(company_id) if company_id else None
            candidate_analysis = await candidate_repository.get_candidate(candidate_id) if candidate_id else None
            culture_fit_result = await culture_fit_result_repository.get_matching_result(matching_id) if matching_id else None

            response_data = {
                "status": "completed",
                "progress": 100,
                "message": "ë¶„ì„ ì™„ë£Œ",
                "company_analysis": company_analysis,
                "candidate_analysis": candidate_analysis,
                "culture_fit_result": culture_fit_result
            }
            return JSONResponse(status_code=200, content=response_data)

        # ì‹¤íŒ¨ ì‹œ 500 Internal Server Error ë°˜í™˜
        if status["status"] == "failed":
            return JSONResponse(status_code=500, content=status)

        await asyncio.sleep(poll_interval)
        elapsed += poll_interval

    # íƒ€ì„ì•„ì›ƒ ì‹œ 202 Accepted (ì•„ì§ ì²˜ë¦¬ ì¤‘) ë°˜í™˜
    return JSONResponse(status_code=202, content=analysis_status.get(result_key))


@router.get("/result/{result_key}")
async def get_result(result_key: str):
    """ìµœì¢… ê²°ê³¼ë§Œ ì¡°íšŒ"""
    if result_key not in analysis_status:
        raise HTTPException(status_code=404, detail="result_key not found")

    status = analysis_status.get(result_key)

    if status["status"] != "completed":
        raise HTTPException(
            status_code=400,
            detail=f"ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ìƒíƒœ: {status['status']}"
        )

    return status["result"]


@router.get("/test")
async def get_test_result():
    company = await company_repository.get_company("6943abdef7b7923a729dc29e")
    candidate = await candidate_repository.get_candidate("6943abaef7b7923a729dc29d")
    result = await culture_fit_result_repository.get_matching_result("6943ae7ca29d0aaf389dc29d")

    return {
        "company_analysis": company,
        "candidate_analysis": candidate,
        "culture_fit_result": result
    }
